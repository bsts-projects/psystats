{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing with *z*-scores and *t*-tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first block of code must be run before anything else.  It only needs to be run once each time you use the website.  It loads the class and class functions necessary to generate the practice problems.  You can hide the first block (it is long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Click to hide code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, math\n",
    "from scipy import stats\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "class RandomData():\n",
    "    def __init__(self, groups = 1, n = 10, distribution = \"normal\"):\n",
    "        self.groups = groups\n",
    "        self.n = n # TODO add option for unequal sample sizes.\n",
    "        self.df = self.generate_data()\n",
    "        self.ss = self.sum_of_squares()\n",
    "        self.means = self.group_means()\n",
    "        self.var = self.variance()\n",
    "        self.std = self.stdev()\n",
    "        self.test = \"\" # value is set when a stats function is called\n",
    "        self.alpha = self.set_alpha()\n",
    "        self.tails = self.set_tails()\n",
    "        self.null = int\n",
    "        self.obt = float\n",
    "        self.crit_values = {}; dict\n",
    "        self.significance = bool\n",
    "        if distribution == \"normal\":\n",
    "            self.distribution = distribution \n",
    "        else:\n",
    "            raise ValueError(\"only the normal distribution is currently supported\")\n",
    "            # TODO add the ability to generate data from other distribuions\n",
    "\n",
    "\n",
    "    def set_alpha(self):\n",
    "        self.alpha = random.choice([0.05, 0.01, 0.001])\n",
    "        return self.alpha\n",
    "        \n",
    "    \n",
    "    def set_tails(self):\n",
    "        self.tails = random.choice([1, 2])\n",
    "        return self.tails\n",
    "            \n",
    "\n",
    "    def generate_data(self):\n",
    "        self.df = pd.DataFrame()\n",
    "        # create data for each group and add it to the dataframe\n",
    "        for group in range(self.groups):\n",
    "            mean = random.randint(10, 100)\n",
    "            sd = mean * random.uniform(0.05, 0.50)\n",
    "\n",
    "            # generate the sample based on the above values\n",
    "            samples = np.random.normal(mean, sd, self.n)\n",
    "\n",
    "            # round the data so it only includes whole numbers\n",
    "            sample = np.round(samples).astype(int)\n",
    "\n",
    "            # convert to a dataframe to display the data\n",
    "            self.df[f'{group}'] = sample\n",
    "        return self.df\n",
    "    \n",
    "\n",
    "    def generate_question(self):\n",
    "        # determine the test type\n",
    "        if self.test == \"z\":\n",
    "            display(Markdown(f\"Given the following data, does $Group_0$ significantly differ from ${{{self.null}}}$?  Use a ${{{self.tails}}}$ tailed-test with $\\\\alpha = {{{self.alpha}}}$\"))\n",
    "            display(Markdown(f\"$M_0 = {{{self.means[0]}}}$\"))\n",
    "            display(Markdown(f\"$ {{\\\\sigma_0}} = {{{round(self.df['0'].std(ddof = 0), 2)}}}$\"))\n",
    "            display(Markdown(f\"$ n = {{{len(self.df['0'])}}}$\"))\n",
    "        elif self.test == \"one-sample t-test\":\n",
    "            display(Markdown(f\"Given the following data, does $Group_0$ significantly differ from ${{{self.null}}}$?  Use a ${{{self.tails}}}$ tailed-test with $\\\\alpha = {{{self.alpha}}}$\"))\n",
    "            display(Markdown(f\"$M_0 = {{{self.means[0]}}}$\"))\n",
    "            display(Markdown(f\"$s^2 = {{{self.var[0]}}}$\"))\n",
    "            display(Markdown(f\"$ n = {{{len(self.df['0'])}}}$\"))\n",
    "        elif self.test == \"independent-samples t-test\":\n",
    "            display(Markdown(f\"Given the following between-subjects data, does $Group_0$ significantly differ from $Group_1$?  Use a ${{{self.tails}}}$ tailed-test with $\\\\alpha = {{{self.alpha}}}$\"))\n",
    "            display(Markdown(f\"$M_0 = {{{self.means[0]}}}, M_1 = {{{self.means[1]}}}$\"))\n",
    "            display(Markdown(f\"$SS_0 = {{{self.ss[0]}}}, SS_1 = {{{self.ss[1]}}}$\"))\n",
    "            display(Markdown(f\"$ n_0 = {{{len(self.df['0'])}}}, n_1 = {{{len(self.df['1'])}}}$\"))\n",
    "        elif self.test == \"dependent-samples t-test\":\n",
    "            display(Markdown(f\"Given the following within-subjects data, does $M_0$ significantly differ from $M_1$?  Use a ${{{self.tails}}}$ tailed-test with $\\\\alpha = {{{self.alpha}}}$\"))\n",
    "            display(Markdown(f\"$M_0 = {{{self.means[0]}}}, M_1 = {{{self.means[1]}}}$\"))\n",
    "            display(Markdown(f\"$ n = {{{len(self.df['0'])}}}$\"))\n",
    "        else:\n",
    "            return ValueError(\"test-type specification error in question geneneration\")\n",
    "        \n",
    "        print(self.df)\n",
    "        \n",
    "\n",
    "    def final_decision(self):\n",
    "        if self.tails == 2:\n",
    "            if self.obt > self.crit_values[\"positive\"] or self.obt < self.crit_values[\"negative\"]:\n",
    "                self.significance = True\n",
    "            else:\n",
    "                self.significance = False\n",
    "        elif self.tails == 1:\n",
    "            if self.crit_values[\"direction\"] == \"increase\" and self.obt > self.crit_values[\"positive\"]:\n",
    "                self.significance = True\n",
    "            elif self.crit_values[\"direction\"] == \"decrease\" and self.obt < self.crit_values[\"negative\"]:\n",
    "                self.significance = True\n",
    "            else:\n",
    "                self.significance = False\n",
    "        else:\n",
    "            return ValueError(\"error in tails specification for final decision\")\n",
    "        \n",
    "        return self.significance \n",
    "    \n",
    "\n",
    "    def write_result(self):\n",
    "        # TODO add more elaborate functionality for the results\n",
    "        if self.test in [\"independent-samples t-test\", \"one-sample t-test\", \"dependent-samples t-test\"]:\n",
    "            # print the critical value for the test\n",
    "            if self.tails == 2:\n",
    "                display(Markdown(f\"$t_{{crit}} = \\\\pm{{{self.crit_values['positive']}}}, \\\\alpha_{{two-tailed}} = {{{self.alpha}}}, df = {{{self.crit_values['degf']}}}$\"))\n",
    "            elif self.tails == 1 and self.crit_values[\"direction\"] == \"increase\":\n",
    "                display(Markdown(f\"$t_{{crit}} = +{{{self.crit_values['positive']}}}, \\\\alpha_{{one-tailed}} = {{{self.alpha}}}, df = {{{self.crit_values['degf']}}}$\"))\n",
    "            elif self.tails == 1 and self.crit_values[\"direction\"] == \"decrease\":\n",
    "                display(Markdown(f\"$t_{{crit}} = {{{self.crit_values['negative']}}}, \\\\alpha_{{one-tailed}} = {{{self.alpha}}}, df = {{{self.crit_values['degf']}}}$\"))\n",
    "            else:\n",
    "                return ValueError(\"tails error in writing results\")\n",
    "            # determine significance\n",
    "            if self.significance:\n",
    "                print(f\"reject the null hypothesis, results are significant, t({self.crit_values['degf']}) = {self.obt}, p < {self.alpha}\")\n",
    "            elif not self.significance:\n",
    "                print(f\"fail to reject the null hypothesis, results not significant, t({self.crit_values['degf']}) = {self.obt}, p > {self.alpha}\")\n",
    "            else:\n",
    "                return ValueError(\"significance boolean error in writing results\")\n",
    "        elif self.test == \"z\":\n",
    "            # print the critical value of t\n",
    "            if self.tails == 2:\n",
    "                display(Markdown(f\"$z_{{crit}} = \\\\pm{{{self.crit_values['positive']}}}, \\\\alpha_{{two-tailed}} = {{{self.alpha}}}$\"))\n",
    "            elif self.tails == 1 and self.crit_values[\"direction\"] == \"increase\":\n",
    "                display(Markdown(f\"$z_{{crit}} = +{{{self.crit_values['positive']}}}, \\\\alpha_{{one-tailed}} = {{{self.alpha}}}$\"))\n",
    "            elif self.tails == 1 and self.crit_values[\"direction\"] == \"decrease\":\n",
    "                display(Markdown(f\"$z_{{crit}} = {{{self.crit_values['negative']}}}, \\\\alpha_{{one-tailed}} = {{{self.alpha}}}$\"))\n",
    "            else:\n",
    "                return ValueError(\"tails error in writing results\")\n",
    "            # determine significance\n",
    "            if self.significance:\n",
    "                print(f\"reject the null hypothesis, results are significant, z = {self.obt}, p < {self.alpha}\")\n",
    "            elif not self.significance:\n",
    "                print(f\"fail to reject the null hypothesis, results not significant, z = {self.obt}, p > {self.alpha}\")\n",
    "            else:\n",
    "                return ValueError(\"significance boolean error in writing results\")\n",
    "        else:\n",
    "            return ValueError(\"test specificaion error when writing results\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def set_null_hypothesis(self):\n",
    "        # for one sample tests, sets a null hypothess between -3 to + 3 x the mean\n",
    "        if  self.test in [\"one-sample t-test\", \"z\"]:\n",
    "            mean = self.means[0]\n",
    "            multiplier = random.uniform(-3, 3)\n",
    "            self.null = round(mean * multiplier)\n",
    "        else:\n",
    "            print(\"else triggered\")\n",
    "            self.null = 0\n",
    "        return self.null\n",
    "\n",
    "\n",
    "    def sum_of_squares(self):  # calculating the values presented in the problem.\n",
    "        ss = []\n",
    "        for column in self.df: \n",
    "            sum_scores = self.df[column].sum()\n",
    "            sum_sqared_scores= (self.df[column].apply(lambda x: x ** 2)).sum()  \n",
    "            ss_vals = sum_sqared_scores - round((sum_scores ** 2)/self.n, 2)\n",
    "            ss.append(round(ss_vals, 2))\n",
    "        return ss\n",
    "\n",
    "\n",
    "    def group_means(self):\n",
    "        means = []\n",
    "        for column in self.df:\n",
    "            means.append(round(self.df[column].mean(), 2))  \n",
    "        return means\n",
    "    \n",
    "\n",
    "    def variance(self):\n",
    "        vars = []\n",
    "        for column in self.df:\n",
    "            vars.append(round(self.df[column].var(ddof = 1), 2))\n",
    "        return vars\n",
    "\n",
    "\n",
    "    def stdev(self):\n",
    "        stdevs = []\n",
    "        for column in self.df:\n",
    "            stdevs.append(round(self.df[column].std(ddof = 1), 2))\n",
    "        return stdevs\n",
    "\n",
    "\n",
    "    def critical_value(self):\n",
    "        # calculate the degrees of freedom based on the type of test used\n",
    "        if self.test == \"independent-samples t-test\":\n",
    "            degf = (self.n - 1) + (self.n - 1)\n",
    "        elif self.test == \"one-sample t-test\" or \"dependent-samples t-test\":\n",
    "            degf = self.n - 1\n",
    "        else:\n",
    "            raise ValueError(\"incorrect test specification - degrees of freedom\")\n",
    "        \n",
    "        # determine the critical values for the inferential test      \n",
    "        if self.tails == 1:\n",
    "            if self.test in [\"independent-samples t-test\", \"one-sample t-test\", \"dependent-samples t-test\"]:\n",
    "                crit = round(stats.t.ppf(1 - self.alpha, degf), 2)\n",
    "            elif self.test == \"z\":\n",
    "                crit = round(stats.norm.ppf(1 - self.alpha), 2)\n",
    "            else:\n",
    "                return ValueError(\"improper test specification - selecting crit values\")\n",
    "        elif self.tails == 2:\n",
    "            if self.test in [\"independent-samples t-test\", \"one-sample t-test\", \"dependent-samples t-test\"]: \n",
    "                crit = round(stats.t.ppf(1 - self.alpha/2, degf), 2)\n",
    "            elif self.test == \"z\":\n",
    "                crit = round(stats.norm.ppf(1 - self.alpha/2), 2)\n",
    "            else:\n",
    "                return ValueError(\"improper test specification - selecting crit values\")\n",
    "        else:\n",
    "            return ValueError(\"self.tails must equal 1 or 2\")\n",
    "        \n",
    "        self.crit_values = {\"positive\": crit, \"negative\": -crit, \"degf\": degf}\n",
    "\n",
    "        # add a direction for one-tailed tests\n",
    "        if self.tails == 1:  \n",
    "            direction = random.choice([\"increase\", \"decrease\"])\n",
    "            self.crit_values[\"direction\"] = direction\n",
    "        else:\n",
    "            return ValueError(\"tails must be 1 for directional crit values\")\n",
    "        \n",
    "        return self.crit_values\n",
    "\n",
    "\n",
    "    def z_test(self):\n",
    "        if len(self.df.columns) > 1:\n",
    "            raise Exception(\"Data contains more than one sample\")\n",
    "        elif len(self.df.columns) == 0:\n",
    "            raise Exception(\"Dataframe error: no data columns\")\n",
    "        else:\n",
    "            self.test = \"z\"\n",
    "\n",
    "            # set the null and write out the question\n",
    "            self.set_null_hypothesis()\n",
    "            self.generate_question()\n",
    "\n",
    "            # calculate the standard error\n",
    "            # TODO double check the work here to make sure it is accurate\n",
    "            sd = round(self.df['0'].std(ddof = 0), 2)\n",
    "            n = len(self.df['0'])\n",
    "            sem = round(sd/(round(math.sqrt(n),2)),2)\n",
    "            self.obt = round((self.means[0] - self.null) / sem, 2)\n",
    "\n",
    "            # TODO add a way to determine environment so output can display in terminal or notebook\n",
    "            # print calculations for the standard error\n",
    "            display(Markdown(\"Calculating the standard error...\"))\n",
    "            display(Markdown(f\"$\\\\sigma_M = \\\\frac{{\\\\sigma}}{{\\\\sqrt{{N}}}}$\"))\n",
    "            display(Markdown(f\"$\\\\sigma_M = \\\\frac{{{sd}}}{{\\\\sqrt{n}}}$\"))\n",
    "            display(Markdown(f\"$\\\\sigma_M = \\\\frac{{{sd}}}{{{round(math.sqrt(n),2)}}}$\"))\n",
    "            display(Markdown(f\"$\\\\sigma_M = {{{sem}}}$\"))\n",
    "            print() # blank space\n",
    "            # print the caluclations for z_obt\n",
    "            display(Markdown(\"calculating $z_{{obt}}$...\"))\n",
    "            display(Markdown(f\"$z_{{obt}} = {{\\\\frac{{M - \\\\mu}}{{\\\\sigma_M}}}}$\"))\n",
    "            display(Markdown(f\"$z_{{obt}} = \\\\frac{{{self.means[0]} - {self.null}}}{{{sem}}}$\"))\n",
    "            display(Markdown(f\"$z_{{obt}} = \\\\frac{{{self.means[0] - self.null}}}{{{sem}}}$\"))\n",
    "            display(Markdown(f\"$z_{{obt}} = {{{self.obt}}}$\"))\n",
    "\n",
    "            self.critical_value()\n",
    "            self.significance = self.final_decision()\n",
    "            self.write_result()\n",
    "\n",
    "            # return self.obt - not returning b/c it was printing out the value of self.obt.  need to figure out why but commenting out fixed it\n",
    "\n",
    "\n",
    "    def one_sample_t_test(self):\n",
    "        if len(self.df.columns) > 1:\n",
    "            raise Exception(\"Data contains more than one sample\")\n",
    "        elif len(self.df.columns) == 0:\n",
    "            raise Exception(\"Dataframe error: no data columns\")\n",
    "        else:\n",
    "            self.test = \"one-sample t-test\"   \n",
    "            \n",
    "            # set the null and write out the question\n",
    "            self.set_null_hypothesis()\n",
    "            self.generate_question()\n",
    "\n",
    "            # calculate the standard error\n",
    "            sem = round(math.sqrt(round((self.var[0]/self.n),2)),2)\n",
    "            self.obt = round((self.means[0] - self.null) / sem, 2)\n",
    "\n",
    "            # print the caluclations for the standard error\n",
    "            # TODO add a way to determine environment so output can display in terminal or notebook\n",
    "            print(\"calculating the standard error...\")\n",
    "            display(Markdown(\"$s_M = \\\\sqrt{{\\\\frac{{s^2}}{{n}}}}$\"))\n",
    "            display(Markdown(f\"$s_M = \\\\sqrt{{\\\\frac{{{self.var[0]}}}{{{self.n}}}}}$\"))\n",
    "            display(Markdown(f\"$s_M = \\\\sqrt{{{round((self.var[0]/self.n),2)}}}$\"))\n",
    "            display(Markdown(f\"$s_M = {{{sem}}}$\"))\n",
    "            print() # blank space\n",
    "            # print the caluclations for t_obt\n",
    "            display(Markdown(\"calculating $t_{{obt}}$...\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = {{\\\\frac{{M - \\\\mu}}{{s_M}}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = \\\\frac{{{self.means[0]} - {self.null}}}{{{sem}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = \\\\frac{{{self.means[0] - self.null}}}{{{sem}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = {{{self.obt}}}$\"))\n",
    "\n",
    "            self.critical_value()\n",
    "            self.significance = self.final_decision()\n",
    "            self.write_result()\n",
    "\n",
    "            # return self.obt - not returning b/c it was printing out the value of self.obt.  need to figure out why but commenting out fixed it        \n",
    "\n",
    "\n",
    "    def independent_samples_t_test(self):\n",
    "        if len(self.df.columns) == 1 or len(self.df.columns) > 2:\n",
    "            raise ValueError(\"Data does not contain two samples\")\n",
    "        elif len(self.df.columns) == 0:\n",
    "            raise ValueError(\"Dataframe error: no data columns\")\n",
    "        else: \n",
    "            self.test = \"independent-samples t-test\"           \n",
    "                        \n",
    "            # set the null and write out the question\n",
    "            self.set_null_hypothesis()\n",
    "            self.generate_question()\n",
    "            \n",
    "            # primary calculations\n",
    "            pooled_var = round(((self.ss[0] + self.ss[1]) / ((self.n - 1) + (self.n - 1))), 2)\n",
    "            sem = round(math.sqrt((round((pooled_var/self.n),2))+(round((pooled_var/self.n),2))),2)\n",
    "            self.obt = round(((self.means[0] - self.means[1]) - self.null) / sem, 2)\n",
    "\n",
    "            # TODO adapt to display in the terminal or a notebook\n",
    "            # display the caluclations for the pooled variance\n",
    "            print(\"calculating the pooled variance...\")\n",
    "            display(Markdown(\"$s_p^2 = {{\\\\frac{{SS_1 + SS_2}}{{df_1 + df_2}}}}$\"))\n",
    "            display(Markdown(f\"$s_p^2 = {{\\\\frac{{{self.ss[0]} + {self.ss[1]}}}{{{self.n - 1} + {self.n - 1}}}}}$\"))\n",
    "            display(Markdown(f\"$s_p^2 = \\\\frac{{{round(self.ss[0] + self.ss[1],2)}}}{{{(self.n - 1) + (self.n - 1)}}}$\"))\n",
    "            display(Markdown(f\"$s_p^2 = {{{pooled_var}}}$\"))\n",
    "            # display the calculations for the estimated standard error\n",
    "            print(\"calculating the estimated standard error of the difference between means...\")\n",
    "            display(Markdown(\"$s_{{(M_1 - M_2)}} = \\\\sqrt{{\\\\frac{{s_p^2}}{{n_1}} + \\\\frac{{s_p^2}}{{n_1}}}}$\"))\n",
    "            display(Markdown(f\"$s_{{(M_1 - M_2)}} = \\\\sqrt{{\\\\frac{{{pooled_var}}}{{{self.n}}} + \\\\frac{{{pooled_var}}}{{{self.n}}}}}$\"))\n",
    "            display(Markdown(f\"$s_{{(M_1 - M_2)}} = \\\\sqrt{{{round(pooled_var/self.n, 2)} + {round(pooled_var/self.n, 2)}}}$\"))\n",
    "            display(Markdown(f\"$s_{{(M_1 - M_2)}} = \\\\sqrt{{{round(pooled_var/self.n, 2) + round(pooled_var/self.n, 2)}}}$\"))\n",
    "            display(Markdown(f\"$s_{{(M_1 - M_2)}} = {{{sem}}}$\"))\n",
    "            # display the caluclations for t_obt\n",
    "            display(Markdown(\"calculating $t_{{obt}}$...\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = {{\\\\frac{{(M_1 - M_2) - (\\\\mu_1 - \\\\mu_2)}}{{s_{{(M_1 - M_2)}}}}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = \\\\frac{{({self.means[0]} - {self.means[1]}) - {{{self.null}}}}}{{{sem}}}$\")) \n",
    "            display(Markdown(f\"$t_{{obt}} = \\\\frac{{{round(self.means[0] - self.means[1] - self.null, 2)}}}{{{sem}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = {{{self.obt}}}$\"))\n",
    "\n",
    "            self.critical_value()\n",
    "            self.significance = self.final_decision()\n",
    "            self.write_result()\n",
    "\n",
    "            # return self.obt - not returning b/c it was printing out the value of self.obt.  need to figure out why but commenting out fixed it\n",
    "        \n",
    "\n",
    "    def dependent_samples_t_test(self):\n",
    "        if len(self.df.columns) == 1 or len(self.df.columns) > 2:\n",
    "            raise ValueError(\"Data does not contain two samples\")\n",
    "        elif len(self.df.columns) == 0:\n",
    "            raise ValueError(\"Dataframe error: no data columns\")\n",
    "        else:\n",
    "            self.test = \"dependent-samples t-test\"     \n",
    "            \n",
    "            # set the null and write out the question\n",
    "            self.set_null_hypothesis()\n",
    "            self.generate_question()\n",
    "\n",
    "            # primary calculations\n",
    "            # need to gather the difference scores\n",
    "            self.df['D'] = self.df['1'] - self.df['0']\n",
    "            \n",
    "            # print the dataframe with the difference scores\n",
    "            display(Markdown(\"Calculating the difference scores $D = X_1 - X_0$\"))\n",
    "            print(self.df.to_string(index = False))\n",
    "            \n",
    "            # Calculate the Mean of the Difference Scores\n",
    "            sum_d = self.df['D'].sum()\n",
    "            n = len(self.df['D'])\n",
    "            mean_d = round(sum_d/n, 2)\n",
    "            display(Markdown(\"Calculating the Mean of the Difference Scores...\"))\n",
    "            display(Markdown(\"$M_D = \\\\frac{{\\\\Sigma D}}{{n}}$\"))\n",
    "            display(Markdown(f\"$M_D = \\\\frac{{{sum_d}}}{{{n}}}$\"))\n",
    "            display(Markdown(f\"$M_D = {{{mean_d}}}$\"))\n",
    "\n",
    "            # calculate the SS for the difference scores\n",
    "            self.df['D^2'] = self.df['D'].apply(lambda x: x ** 2)\n",
    "            sum_sqared_scores = self.df['D^2'].sum()\n",
    "            ss = round(sum_sqared_scores - round((sum_d ** 2)/n, 2), 2)\n",
    "            # print the dataframe with the squared difference scores\n",
    "            display(Markdown(\"Calculating the sum of the squared deviations...\"))\n",
    "            print(self.df.to_string(index = False))\n",
    "            display(Markdown(\"$ SS_D = \\\\Sigma D^2 - \\\\frac{{(\\\\Sigma D)^2}}{{n}}$\"))\n",
    "            display(Markdown(f\"$ SS_D = {{{sum_sqared_scores}}} - \\\\frac{{{sum_d ** 2}}}{{{n}}}$\"))\n",
    "            display(Markdown(f\"$ SS_D = {{{sum_sqared_scores}}} - {{{round((sum_d ** 2)/n, 2)}}}$\"))\n",
    "            display(Markdown(f\"$ SS_D = {{{ss}}}$\"))\n",
    "\n",
    "            # calculate the variance    \n",
    "            variance = round(ss / (n - 1), 2)\n",
    "            display(Markdown(\"$ s^2 = \\\\frac{{SS_D}}{{n}}$\")) \n",
    "            display(Markdown(f\"$ s^2 = \\\\frac{{{ss}}}{{{n}}}$\"))   \n",
    "            display(Markdown(f\"$ s^2 = \\\\frac{{{round(ss/n, 2)}}}$\"))\n",
    "            display(Markdown(f\"$ s^2 = {{{variance}}}$\"))  \n",
    "\n",
    "            # Calculate the estimated standard error\n",
    "            sem = round(math.sqrt(variance/n), 2)\n",
    "            display(Markdown(\"Calculating the estimated standard error...\"))\n",
    "            display(Markdown(\"$ s_{M_D} = \\\\sqrt{{\\\\frac{{s^2}}{{n}}}}$\"))\n",
    "            display(Markdown(f\"$ s_{{M_D}} = \\\\sqrt{{\\\\frac{{{variance}}}{{{n}}}}}$\"))\n",
    "            display(Markdown(f\"$ s_{{M_D}} = \\\\sqrt{{{round(variance/n, 2)}}}$\"))\n",
    "            display(Markdown(f\"$ s_{{M_D}} = {{{sem}}}$\"))\n",
    "\n",
    "            # caclulate the t-statistic\n",
    "            self.obt = round((mean_d - self.null) / sem, 2)\n",
    "            display(Markdown(\"calculating $t_{{obt}}$...\"))\n",
    "            display(Markdown(\"$t_{{obt}} = {{\\\\frac{{M_D - \\\\mu_D}}{{s_{M_D}}}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = \\\\frac{{{mean_d} - {self.null}}}{{{sem}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = \\\\frac{{{mean_d - self.null}}}{{{sem}}}$\"))\n",
    "            display(Markdown(f\"$t_{{obt}} = {{{self.obt}}}$\"))\n",
    "\n",
    "            self.critical_value()\n",
    "            self.significance = self.final_decision()\n",
    "            self.write_result()\n",
    "\n",
    "            # return self.obt - not returning b/c it was printing out the value of self.obt.  need to figure out why but commenting out fixed it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *z*-test\n",
    "\n",
    "conducting a *z*-test.  Groups needs to be set to 1.  The value for n can be any integer (whole number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomData(groups = 1, n = 10).z_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Sample *t*-test\n",
    "\n",
    "conducting a *t*-test with one sample.  Groups needs to be set to 1.  The value for n can be any integer (whole number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomData(groups = 1, n = 10).one_sample_t_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent-Samples *t*-test\n",
    "\n",
    "conducting a *t*-test with two independent samples.  Groups needs to be set to 2.  The value for n can be any integer (whole number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomData(groups = 2, n = 10).independent_samples_t_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent-Samples *t*-test\n",
    "\n",
    "conducting a *t*-test with two dependent samples.  Groups needs to be set to 2.  The value for n can be any integer (whole number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomData(groups = 2, n = 10).dependent_samples_t_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
